#!/usr/bin/env tsx

import { Command } from 'commander'
import { CrawlerManager, CrawlerSource } from '@/lib/crawlers/crawler-manager'
import chalk from 'chalk'
import { CrawlerOptions } from '@/lib/crawlers/base-hotdeal-crawler'

const program = new Command()

program
  .name('crawl-hotdeals')
  .description('í•œêµ­ í•«ë”œ í¬ë¡¤ëŸ¬ - ë½ë¿Œ ë“± ì»¤ë®¤ë‹ˆí‹°ì—ì„œ í•«ë”œ ì •ë³´ ìˆ˜ì§‘')
  .version('1.0.0')

program
  .option('-s, --source <source>', 'í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ (ppomppu, ruliweb, clien, quasarzone, coolenjoy, itcm, all)', 'ppomppu')
  .option('-p, --pages <number>', 'í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜', '2')
  .option('-o, --output <directory>', 'ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ', './exports')
  .option('--headless <boolean>', 'í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€', 'true')
  .option('-d, --delay <ms>', 'í˜ì´ì§€ ê°„ ë”œë ˆì´ (ë°€ë¦¬ì´ˆ)', '2000')
  .option('--save-db', 'ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥', false)
  .option('--save-json', 'JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°', true)
  .option('--group-by-source', 'ì†ŒìŠ¤ë³„ë¡œ ë³„ë„ íŒŒì¼ ìƒì„±', false)
  .option('--import <filepath>', 'JSON íŒŒì¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°')
  .action(async (options) => {
    console.log(chalk.blue('ğŸš€ í•œêµ­ í•«ë”œ í¬ë¡¤ëŸ¬ ì‹œì‘\n'))
    
    // Parse options
    const crawlerOptions: CrawlerOptions = {
      headless: options.headless === 'true',
      maxPages: parseInt(options.pages),
      delay: parseInt(options.delay)
    }
    
    console.log(chalk.gray('ğŸ“‹ ì„¤ì •:'))
    console.log(chalk.gray(`- ì‚¬ì´íŠ¸: ${options.source}`))
    console.log(chalk.gray(`- í˜ì´ì§€: ${crawlerOptions.maxPages}`))
    console.log(chalk.gray(`- ì¶œë ¥: ${options.output}`))
    console.log(chalk.gray(`- í—¤ë“œë¦¬ìŠ¤: ${crawlerOptions.headless}`))
    console.log(chalk.gray(`- DB ì €ì¥: ${options.saveDb}`))
    console.log(chalk.gray(`- JSON ì €ì¥: ${options.saveJson}`))
    
    try {
      const manager = new CrawlerManager(crawlerOptions)
      
      // Import from JSON if specified
      if (options.import) {
        const hotdeals = await manager.importFromJson(options.import)
        
        if (options.saveDb) {
          const results = [{
            source: 'imported',
            hotdeals,
            totalCrawled: hotdeals.length,
            newDeals: hotdeals.length,
            updatedDeals: 0,
            errors: 0,
            duration: 0,
            statistics: {},
            crawledAt: new Date().toISOString()
          }]
          await manager.saveToDatabase(results)
        }
        
        return
      }
      
      // Crawl specified sources
      const results = await manager.crawl(options.source as CrawlerSource)
      
      // Save to database if requested
      if (options.saveDb) {
        await manager.saveToDatabase(results)
      }
      
      // Export to JSON if requested
      if (options.saveJson) {
        const files = await manager.exportToJson(
          results, 
          options.output, 
          options.groupBySource
        )
        
        console.log(chalk.green('\nâœ… í¬ë¡¤ë§ ì™„ë£Œ!'))
        console.log(chalk.gray(`ë‚´ë³´ë‚¸ íŒŒì¼: ${files.length}ê°œ`))
      }
      
    } catch (error) {
      console.error(chalk.red('\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨:'), error)
      process.exit(1)
    }
  })

// Run examples
program.on('--help', () => {
  console.log('')
  console.log('Examples:')
  console.log('  $ pnpm crawl-hotdeals')
  console.log('  $ pnpm crawl-hotdeals -p 5 -o ./data')
  console.log('  $ pnpm crawl-hotdeals --headless false')
  console.log('  $ pnpm crawl-hotdeals --save-db --save-json')
  console.log('  $ pnpm crawl-hotdeals --import ./exports/hotdeal-ppomppu-2025-07-12.json --save-db')
})

program.parse(process.argv)