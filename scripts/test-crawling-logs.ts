import 'dotenv/config'
import { supabaseAdmin } from '../lib/supabase/client'
import type { Database } from '../database.types'

type CrawlingLogInsert = Database['public']['Tables']['crawling_logs']['Insert']
type CrawlingLogRow = Database['public']['Tables']['crawling_logs']['Row']

async function testCrawlingLogs() {
  console.log('ğŸ§ª í¬ë¡¤ë§ ë¡œê·¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n')

  const client = supabaseAdmin()
  if (!client) {
    console.error('âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨')
    console.error('í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:')
    console.error('- NEXT_PUBLIC_SUPABASE_URL:', process.env.NEXT_PUBLIC_SUPABASE_URL ? 'âœ…' : 'âŒ')
    console.error('- SUPABASE_SERVICE_ROLE_KEY:', process.env.SUPABASE_SERVICE_ROLE_KEY ? 'âœ…' : 'âŒ')
    return
  }

  console.log('âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ')

  // 1. crawling_logs í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸
  console.log('\n1ï¸âƒ£ crawling_logs í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ í™•ì¸')
  const { data: existingLogs, error: schemaError } = await client
    .from('crawling_logs')
    .select('*')
    .limit(1)

  if (schemaError) {
    console.error('âŒ crawling_logs í…Œì´ë¸” ì ‘ê·¼ ì‹¤íŒ¨:', schemaError.message)
    return
  }

  console.log('âœ… crawling_logs í…Œì´ë¸” ì ‘ê·¼ ì„±ê³µ')
  if (existingLogs && existingLogs.length > 0) {
    console.log('   ê¸°ì¡´ ë¡œê·¸ ìƒ˜í”Œ:', existingLogs[0])
  } else {
    console.log('   ê¸°ì¡´ ë¡œê·¸ ì—†ìŒ (ì²« ì‹¤í–‰)')
  }

  // 2. í¬ë¡¤ë§ ë¡œê·¸ ìƒì„± í…ŒìŠ¤íŠ¸
  console.log('\n2ï¸âƒ£ í¬ë¡¤ë§ ë¡œê·¸ ìƒì„± í…ŒìŠ¤íŠ¸')
  const testLogData: CrawlingLogInsert = {
    source: 'ppomppu',
    status: 'running',
    started_at: new Date().toISOString(),
    items_found: 0,
    items_added: 0,
    items_updated: 0,
    duplicates: 0
  }

  const { data: createdLog, error: createError } = await client
    .from('crawling_logs')
    .insert(testLogData)
    .select()
    .single()

  if (createError || !createdLog) {
    console.error('âŒ í¬ë¡¤ë§ ë¡œê·¸ ìƒì„± ì‹¤íŒ¨:', createError)
    return
  }

  console.log('âœ… í¬ë¡¤ë§ ë¡œê·¸ ìƒì„± ì„±ê³µ!')
  console.log('   ë¡œê·¸ ID:', createdLog.id)
  console.log('   ì†ŒìŠ¤:', createdLog.source)
  console.log('   ìƒíƒœ:', createdLog.status)
  console.log('   ì‹œì‘ ì‹œê°„:', createdLog.started_at)

  // 3. í¬ë¡¤ë§ ë¡œê·¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸ (ì™„ë£Œ ì²˜ë¦¬)
  console.log('\n3ï¸âƒ£ í¬ë¡¤ë§ ë¡œê·¸ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸')
  const completedAt = new Date()
  const startedAt = new Date(createdLog.started_at)
  const durationMs = completedAt.getTime() - startedAt.getTime()

  const { data: updatedLog, error: updateError } = await client
    .from('crawling_logs')
    .update({
      status: 'completed',
      completed_at: completedAt.toISOString(),
      duration_ms: durationMs,
      items_found: 25,
      items_added: 15,
      items_updated: 5,
      duplicates: 5
    })
    .eq('id', createdLog.id)
    .select()
    .single()

  if (updateError || !updatedLog) {
    console.error('âŒ í¬ë¡¤ë§ ë¡œê·¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:', updateError)
    return
  }

  console.log('âœ… í¬ë¡¤ë§ ë¡œê·¸ ì—…ë°ì´íŠ¸ ì„±ê³µ!')
  console.log('   ìƒíƒœ:', updatedLog.status)
  console.log('   ì™„ë£Œ ì‹œê°„:', updatedLog.completed_at)
  console.log('   ì†Œìš” ì‹œê°„:', updatedLog.duration_ms, 'ms')
  console.log('   ë°œê²¬ ì•„ì´í…œ:', updatedLog.items_found)
  console.log('   ì¶”ê°€ ì•„ì´í…œ:', updatedLog.items_added)
  console.log('   ì—…ë°ì´íŠ¸ ì•„ì´í…œ:', updatedLog.items_updated)
  console.log('   ì¤‘ë³µ ì•„ì´í…œ:', updatedLog.duplicates)

  // 4. ì—ëŸ¬ ë¡œê·¸ í…ŒìŠ¤íŠ¸
  console.log('\n4ï¸âƒ£ ì—ëŸ¬ ë¡œê·¸ í…ŒìŠ¤íŠ¸')
  const errorLogData: CrawlingLogInsert = {
    source: 'ruliweb',
    status: 'failed',
    started_at: new Date().toISOString(),
    completed_at: new Date().toISOString(),
    duration_ms: 5000,
    items_found: 0,
    items_added: 0,
    items_updated: 0,
    duplicates: 0,
    error_message: 'í…ŒìŠ¤íŠ¸ ì—ëŸ¬: ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨',
    error_details: {
      code: 'NETWORK_ERROR',
      url: 'https://bbs.ruliweb.com/market/board/1020',
      stack: 'Error at testCrawler line 123'
    }
  }

  const { data: errorLog, error: errorCreateError } = await client
    .from('crawling_logs')
    .insert(errorLogData)
    .select()
    .single()

  if (errorCreateError || !errorLog) {
    console.error('âŒ ì—ëŸ¬ ë¡œê·¸ ìƒì„± ì‹¤íŒ¨:', errorCreateError)
    return
  }

  console.log('âœ… ì—ëŸ¬ ë¡œê·¸ ìƒì„± ì„±ê³µ!')
  console.log('   ë¡œê·¸ ID:', errorLog.id)
  console.log('   ìƒíƒœ:', errorLog.status)
  console.log('   ì—ëŸ¬ ë©”ì‹œì§€:', errorLog.error_message)
  console.log('   ì—ëŸ¬ ìƒì„¸:', errorLog.error_details)

  // 5. ë¡œê·¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸
  console.log('\n5ï¸âƒ£ ë¡œê·¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸')
  const { data: allLogs, error: listError } = await client
    .from('crawling_logs')
    .select('*')
    .order('started_at', { ascending: false })
    .limit(5)

  if (listError) {
    console.error('âŒ ë¡œê·¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨:', listError)
    return
  }

  console.log(`âœ… ìµœê·¼ ${allLogs?.length || 0}ê°œ ë¡œê·¸ ì¡°íšŒ ì„±ê³µ!`)
  allLogs?.forEach((log, index) => {
    console.log(`   ${index + 1}. [${log.source}] ${log.status} - ${log.started_at}`)
  })

  // 6. íŠ¹ì • ì†ŒìŠ¤ë³„ í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸
  console.log('\n6ï¸âƒ£ ì†ŒìŠ¤ë³„ í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸')
  const { data: ppomppuLogs, error: statsError } = await client
    .from('crawling_logs')
    .select('status, items_added, items_updated, duplicates')
    .eq('source', 'ppomppu')
    .eq('status', 'completed')

  if (statsError) {
    console.error('âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨:', statsError)
  } else {
    const totalAdded = ppomppuLogs?.reduce((sum, log) => sum + (log.items_added || 0), 0) || 0
    const totalUpdated = ppomppuLogs?.reduce((sum, log) => sum + (log.items_updated || 0), 0) || 0
    const totalDuplicates = ppomppuLogs?.reduce((sum, log) => sum + (log.duplicates || 0), 0) || 0
    
    console.log('âœ… ë½ë¿Œ í¬ë¡¤ë§ í†µê³„:')
    console.log(`   ì™„ë£Œëœ í¬ë¡¤ë§: ${ppomppuLogs?.length || 0}íšŒ`)
    console.log(`   ì´ ì¶”ê°€ ì•„ì´í…œ: ${totalAdded}ê°œ`)
    console.log(`   ì´ ì—…ë°ì´íŠ¸ ì•„ì´í…œ: ${totalUpdated}ê°œ`)
    console.log(`   ì´ ì¤‘ë³µ ì•„ì´í…œ: ${totalDuplicates}ê°œ`)
  }

  // 7. ì •ë¦¬ - í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚­ì œ
  console.log('\n7ï¸âƒ£ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬')
  const testLogIds = [createdLog.id, errorLog.id]
  
  const { error: cleanupError } = await client
    .from('crawling_logs')
    .delete()
    .in('id', testLogIds)

  if (cleanupError) {
    console.error('âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨:', cleanupError)
  } else {
    console.log('âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ')
  }

  console.log('\nâœ… í¬ë¡¤ë§ ë¡œê·¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
}

// ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
testCrawlingLogs()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error)
    process.exit(1)
  })