require('dotenv').config({ path: '.env.local' });

console.log('ğŸŒ í™˜ê²½ë³€ìˆ˜ í™•ì¸:');
console.log('SUPABASE_URL:', !!process.env.NEXT_PUBLIC_SUPABASE_URL);
console.log('SUPABASE_KEY:', !!process.env.SUPABASE_SERVICE_ROLE_KEY);

const { crawlerScheduler } = require('./lib/services/crawler-scheduler.ts');

async function testCrawler() {
  console.log('ğŸ•·ï¸ ìˆ˜ë™ í¬ë¡¤ë§ ì‹œì‘ (Supabase ëª¨ë“œ)...');
  
  try {
    const result = await crawlerScheduler.runCrawlManually('ppomppu', {
      maxPages: 2,
      timeFilterHours: 12  // ìµœê·¼ 12ì‹œê°„ ì´ë‚´
    });
    
    console.log('âœ… í¬ë¡¤ë§ ì™„ë£Œ!');
    console.log('ğŸ“Š ê²°ê³¼:');
    console.log('  - ì´ ìˆ˜ì§‘:', result.totalCrawled);
    console.log('  - ì‹ ê·œ:', result.newDeals);
    console.log('  - ì—…ë°ì´íŠ¸:', result.updatedDeals);
    
    if (result.results && result.results.length > 0) {
      console.log('\nğŸ“‹ ìˆ˜ì§‘ëœ í•«ë”œ ìƒ˜í”Œ:');
      result.results[0].hotdeals.slice(0, 3).forEach((deal, index) => {
        console.log(`${index + 1}. ${deal.title} (${deal.source})`);
      });
    }
    
  } catch (error) {
    console.error('âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:', error);
  }
}

testCrawler();