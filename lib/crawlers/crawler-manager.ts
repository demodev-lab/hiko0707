import { PpomppuCrawler } from './ppomppu-crawler'
import { RuliwebCrawler } from './ruliweb-crawler'
import { QuasarzoneCrawler } from './quasarzone-crawler'
import { ClienCrawler } from './clien-crawler'
import { EomisaeCrawler } from './eomisae-crawler'
import { CoolenjoyCrawler } from './coolenjoy-crawler'
import { CrawlerOptions } from './base-hotdeal-crawler'
import type { CrawlResult } from './types'
import { SupabaseHotDealService } from '@/lib/services/supabase-hotdeal-service'
import { writeFile, readFile, mkdir } from 'fs/promises'
import { existsSync } from 'fs'
import { join } from 'path'
import chalk from 'chalk'

export type CrawlerSource = 'ppomppu' | 'ruliweb' | 'clien' | 'quasarzone' | 'eomisae' | 'coolenjoy' | 'itcm'

export interface CrawlJobOptions {
  sources: CrawlerSource[]
  maxPages?: number
  pageDelay?: number
  detailDelay?: number
  skipDetail?: boolean
  concurrent?: boolean
  retryAttempts?: number
  retryDelay?: number
}

export interface CrawlJobResult {
  success: boolean
  stats: {
    totalCrawled: number
    totalSaved: number
    totalErrors: number
    duration: number
  }
  errors?: Map<string, string[]>
}

export class CrawlerManager {
  private options: CrawlerOptions
  
  constructor(options: CrawlerOptions = {}) {
    this.options = {
      headless: options.headless ?? true,
      maxPages: options.maxPages ?? 2,
      delay: options.delay ?? 3000,
      timeout: options.timeout ?? 60000,
      viewport: options.viewport ?? { width: 1920, height: 1080 },
      userAgent: options.userAgent ?? 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
      timeFilterHours: options.timeFilterHours,
      onProgress: options.onProgress
    }
  }

  async crawl(source: CrawlerSource | CrawlerSource[]): Promise<CrawlResult[]> {
    const sources = Array.isArray(source) ? source : [source]
    const results: CrawlResult[] = []
    
    console.log(chalk.cyan(`ğŸ•·ï¸  í¬ë¡¤ë§ ì‹œì‘: ${sources.join(', ')}`))
    const startTime = Date.now()
    
    for (const src of sources) {
      try {
        console.log(chalk.yellow(`\nğŸ“Œ ${src} í¬ë¡¤ë§ ì‹œì‘...`))
        
        const crawler = this.createCrawler(src)
        const result = await crawler.crawl()
        
        results.push(result)
        
        console.log(chalk.green(`âœ… ${src} í¬ë¡¤ë§ ì™„ë£Œ`))
        console.log(chalk.gray(`   - ì´ ìˆ˜ì§‘: ${result.totalCrawled}ê°œ`))
        console.log(chalk.gray(`   - ì‹ ê·œ: ${result.newDeals}ê°œ`))
        console.log(chalk.gray(`   - ì—…ë°ì´íŠ¸: ${result.updatedDeals}ê°œ`))
        console.log(chalk.gray(`   - ì˜¤ë¥˜: ${result.errors}ê°œ`))
        
        // ë‹¤ìŒ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì „ ë”œë ˆì´
        if (src !== sources[sources.length - 1]) {
          await this.delay(5000)
        }
      } catch (error) {
        console.error(chalk.red(`âŒ ${src} í¬ë¡¤ë§ ì‹¤íŒ¨:`), error)
        
        // ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê²°ê³¼ ì¶”ê°€ (ì—ëŸ¬ ì •ë³´ í¬í•¨)
        results.push({
          totalCrawled: 0,
          newDeals: 0,
          updatedDeals: 0,
          errors: 1,
          duration: 0,
          hotdeals: []
        })
      }
    }
    
    const totalDuration = Date.now() - startTime
    
    // ì „ì²´ í†µê³„ ì¶œë ¥
    console.log(chalk.cyan('\nğŸ“Š ì „ì²´ í¬ë¡¤ë§ í†µê³„:'))
    console.log(chalk.gray(`- í¬ë¡¤ë§ ì‚¬ì´íŠ¸: ${sources.length}ê°œ`))
    console.log(chalk.gray(`- ì´ ìˆ˜ì§‘: ${results.reduce((sum, r) => sum + r.totalCrawled, 0)}ê°œ`))
    console.log(chalk.gray(`- ì´ ì‹ ê·œ: ${results.reduce((sum, r) => sum + r.newDeals, 0)}ê°œ`))
    console.log(chalk.gray(`- ì´ ì—…ë°ì´íŠ¸: ${results.reduce((sum, r) => sum + r.updatedDeals, 0)}ê°œ`))
    console.log(chalk.gray(`- ì´ ì˜¤ë¥˜: ${results.reduce((sum, r) => sum + r.errors, 0)}ê°œ`))
    console.log(chalk.gray(`- ì´ ì†Œìš”ì‹œê°„: ${totalDuration}ms`))
    
    return results
  }

  private createCrawler(source: CrawlerSource) {
    switch (source) {
      case 'ppomppu':
        return new PpomppuCrawler(this.options)
      case 'ruliweb':
        return new RuliwebCrawler(this.options)
      case 'quasarzone':
        return new QuasarzoneCrawler(this.options)
      case 'clien':
        return new ClienCrawler(this.options)
      case 'eomisae':
        return new EomisaeCrawler(this.options)
      case 'coolenjoy':
        return new CoolenjoyCrawler(this.options)
      default:
        throw new Error(`ì•Œ ìˆ˜ ì—†ëŠ” í¬ë¡¤ëŸ¬ ì†ŒìŠ¤: ${source}`)
    }
  }

  private async delay(ms: number): Promise<void> {
    await new Promise(resolve => setTimeout(resolve, ms))
  }

  /**
   * í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
   */
  async saveToDatabase(results: CrawlResult[]): Promise<void> {
    console.log(chalk.cyan('\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...'))
    
    for (const result of results) {
      if (result.hotdeals.length > 0) {
        try {
          const importResult = await SupabaseHotDealService.importFromCrawler(
            result.hotdeals[0]?.source || 'unknown',
            result.hotdeals
          )
          
          console.log(chalk.green(`âœ… ${result.hotdeals[0]?.source} ì €ì¥ ì™„ë£Œ:`))
          console.log(chalk.gray(`   - ì‹ ê·œ: ${importResult.added}ê°œ`))
          console.log(chalk.gray(`   - ì—…ë°ì´íŠ¸: ${importResult.updated}ê°œ`))
          console.log(chalk.gray(`   - ì˜¤ë¥˜: ${importResult.errors.length}ê°œ`))
        } catch (error) {
          console.error(chalk.red(`âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨:`), error)
        }
      }
    }
  }

  /**
   * í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
   */
  async exportToJson(
    results: CrawlResult[], 
    outputDir: string, 
    groupBySource: boolean = false
  ): Promise<string[]> {
    console.log(chalk.cyan('\nğŸ“ JSON íŒŒì¼ ë‚´ë³´ë‚´ê¸°...'))
    
    // ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if (!existsSync(outputDir)) {
      await mkdir(outputDir, { recursive: true })
    }

    const files: string[] = []
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-')

    if (groupBySource) {
      // ì†ŒìŠ¤ë³„ë¡œ ë³„ê°œ íŒŒì¼ ìƒì„±
      for (const result of results) {
        if (result.hotdeals.length > 0) {
          const source = result.hotdeals[0]?.source || 'unknown'
          const filename = `hotdeal-${source}-${timestamp}.json`
          const filepath = join(outputDir, filename)
          
          await writeFile(filepath, JSON.stringify({
            source,
            timestamp: new Date().toISOString(),
            count: result.hotdeals.length,
            hotdeals: result.hotdeals
          }, null, 2))
          
          files.push(filepath)
          console.log(chalk.green(`âœ… ${filename} ìƒì„± ì™„ë£Œ (${result.hotdeals.length}ê°œ ë”œ)`))
        }
      }
    } else {
      // ëª¨ë“  ì†ŒìŠ¤ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ í•©ì¹˜ê¸°
      const allHotdeals = results.flatMap(r => r.hotdeals)
      const filename = `hotdeal-all-${timestamp}.json`
      const filepath = join(outputDir, filename)
      
      await writeFile(filepath, JSON.stringify({
        timestamp: new Date().toISOString(),
        sources: results.length,
        count: allHotdeals.length,
        hotdeals: allHotdeals
      }, null, 2))
      
      files.push(filepath)
      console.log(chalk.green(`âœ… ${filename} ìƒì„± ì™„ë£Œ (${allHotdeals.length}ê°œ ë”œ)`))
    }

    return files
  }

  /**
   * JSON íŒŒì¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
   */
  async importFromJson(filepath: string): Promise<any[]> {
    console.log(chalk.cyan(`ğŸ“‚ JSON íŒŒì¼ ê°€ì ¸ì˜¤ê¸°: ${filepath}`))
    
    try {
      const content = await readFile(filepath, 'utf-8')
      const data = JSON.parse(content)
      
      const hotdeals = data.hotdeals || []
      console.log(chalk.green(`âœ… ${hotdeals.length}ê°œ ë”œ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ`))
      
      return hotdeals
    } catch (error) {
      console.error(chalk.red(`âŒ JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨:`), error)
      return []
    }
  }

  // ===== ì •ì  ë©”ì„œë“œë“¤ =====

  /**
   * í¬ë¡¤ ì‘ì—… ì‹¤í–‰ (ì •ì  ë©”ì„œë“œ)
   */
  static async executeCrawlJob(options: CrawlJobOptions): Promise<CrawlJobResult> {
    const startTime = Date.now()
    let totalCrawled = 0
    let totalSaved = 0
    let totalErrors = 0
    const errors = new Map<string, string[]>()

    try {
      const manager = new CrawlerManager({
        maxPages: options.maxPages || 2,
        delay: options.pageDelay || 3000
      })

      const results = await manager.crawl(options.sources)
      
      // í†µê³„ ì§‘ê³„
      for (const result of results) {
        totalCrawled += result.totalCrawled
        totalSaved += result.newDeals + result.updatedDeals
        totalErrors += result.errors
      }

      // ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
      if (totalCrawled > 0) {
        await manager.saveToDatabase(results)
      }

      const duration = Date.now() - startTime

      return {
        success: totalErrors === 0,
        stats: {
          totalCrawled,
          totalSaved,
          totalErrors,
          duration
        },
        errors: errors.size > 0 ? errors : undefined
      }
    } catch (error) {
      const duration = Date.now() - startTime
      
      return {
        success: false,
        stats: {
          totalCrawled,
          totalSaved,
          totalErrors: totalErrors + 1,
          duration
        },
        errors: new Map([['general', [String(error)]]])
      }
    }
  }

  /**
   * í¬ë¡¤ëŸ¬ ìƒíƒœ ì¡°íšŒ (ì •ì  ë©”ì„œë“œ)
   */
  static async getCrawlerStatus(): Promise<{
    isRunning: boolean
    lastRun?: Date
    availableCrawlers: string[]
    runningJobs: string[]
  }> {
    return {
      isRunning: false, // ì‹¤ì œ êµ¬í˜„ ì‹œ ìƒíƒœ ì²´í¬ ë¡œì§ ì¶”ê°€
      availableCrawlers: ['ppomppu', 'ruliweb', 'quasarzone', 'clien', 'eomisae', 'coolenjoy'],
      runningJobs: []
    }
  }
}