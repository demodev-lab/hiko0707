import { BaseHotdealCrawler, CrawlerOptions } from './base-hotdeal-crawler'
import { PpomppuCrawler } from './new-ppomppu-crawler'
import type { HotDeal } from '@/types/hotdeal'
import { db } from '@/lib/db/database-service'
import fs from 'fs/promises'
import path from 'path'
import chalk from 'chalk'

export type CrawlerSource = 'ppomppu' | 'ruliweb' | 'clien' | 'quasarzone' | 'coolenjoy' | 'itcm' | 'all'

interface CrawlerResult {
  source: string
  hotdeals: HotDeal[]
  statistics: any
  crawledAt: string
}

interface ExportMetadata {
  exportDate: string
  totalDeals: number
  source: string
  version: string
  exportedBy: string
  crawledPages: number
}

export class HotdealCrawlerManager {
  private crawlers: Map<string, BaseHotdealCrawler>
  private options: CrawlerOptions

  constructor(options: CrawlerOptions = {}) {
    this.options = options
    this.crawlers = new Map()
    this.initializeCrawlers()
  }

  private initializeCrawlers(): void {
    // Initialize available crawlers
    this.crawlers.set('ppomppu', new PpomppuCrawler(this.options))
    
    // TODO: Add other crawlers as they are implemented
    // this.crawlers.set('ruliweb', new RuliwebCrawler(this.options))
    // this.crawlers.set('clien', new ClienCrawler(this.options))
    // etc.
  }

  setProgressCallback(callback: (current: number, total: number, step: string) => void): void {
    this.options.onProgress = callback
    // ê¸°ì¡´ í¬ë¡¤ëŸ¬ë“¤ì— ìƒˆë¡œìš´ ì˜µì…˜ ì ìš©ì„ ìœ„í•´ ì¬ì´ˆê¸°í™”
    this.crawlers.clear()
    this.initializeCrawlers()
  }

  async crawl(source: CrawlerSource = 'ppomppu'): Promise<CrawlerResult[]> {
    const results: CrawlerResult[] = []
    
    if (source === 'all') {
      // Crawl all available sources
      for (const [sourceName, crawler] of this.crawlers) {
        try {
          console.log(chalk.blue(`\nğŸš€ í¬ë¡¤ë§ ì‹œì‘: ${sourceName}`))
          const hotdeals = await crawler.crawl()
          const statistics = (crawler as any).generateStatistics()
          
          results.push({
            source: sourceName,
            hotdeals,
            statistics,
            crawledAt: new Date().toISOString()
          })
        } catch (error) {
          console.error(chalk.red(`âŒ ${sourceName} í¬ë¡¤ë§ ì‹¤íŒ¨:`, error))
        }
      }
    } else {
      // Crawl specific source
      const crawler = this.crawlers.get(source)
      if (!crawler) {
        throw new Error(`Crawler for source '${source}' not found`)
      }
      
      const hotdeals = await crawler.crawl()
      const statistics = (crawler as any).generateStatistics()
      
      results.push({
        source,
        hotdeals,
        statistics,
        crawledAt: new Date().toISOString()
      })
    }
    
    return results
  }

  async saveToDatabase(results: CrawlerResult[]): Promise<void> {
    console.log(chalk.cyan('\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...'))
    
    // ì„œë²„ í™˜ê²½ì—ì„œëŠ” ì§ì ‘ ì €ì¥ì´ ì–´ë ¤ìš°ë¯€ë¡œ ë¡œê·¸ë§Œ ì¶œë ¥
    for (const result of results) {
      console.log(chalk.yellow(`âš ï¸  ${result.source}: ${result.hotdeals.length}ê°œ í•«ë”œ (ë¸Œë¼ìš°ì €ì—ì„œ ì €ì¥ í•„ìš”)`))
    }
    
    console.log(chalk.yellow('\nğŸ’¡ ì›¹ ì¸í„°í˜ì´ìŠ¤ì—ì„œ "DB ì €ì¥" ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.'))
  }

  async exportToJson(
    results: CrawlerResult[], 
    outputDir: string = './exports',
    groupBySource: boolean = false
  ): Promise<string[]> {
    console.log(chalk.cyan('\nğŸ“ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°...'))
    
    // Ensure output directory exists
    await fs.mkdir(outputDir, { recursive: true })
    
    const exportedFiles: string[] = []
    
    if (groupBySource) {
      // Export separate file for each source
      for (const result of results) {
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
        const filename = `hotdeal-${result.source}-${timestamp}.json`
        const filepath = path.join(outputDir, filename)
        
        const exportData = {
          metadata: this.createMetadata(result),
          hotdeals: result.hotdeals,
          statistics: result.statistics
        }
        
        await fs.writeFile(filepath, JSON.stringify(exportData, null, 2), 'utf-8')
        exportedFiles.push(filepath)
        
        const stats = await fs.stat(filepath)
        const sizeKB = Math.round(stats.size / 1024)
        
        console.log(chalk.green(`âœ… ${result.source} ë‚´ë³´ë‚´ê¸° ì™„ë£Œ`))
        console.log(chalk.gray(`   íŒŒì¼: ${filepath}`))
        console.log(chalk.gray(`   í¬ê¸°: ${sizeKB}KB`))
        console.log(chalk.gray(`   ë”œ ìˆ˜: ${result.hotdeals.length}ê°œ`))
      }
    } else {
      // Export all results to single file
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-')
      const filename = `hotdeal-all-${timestamp}.json`
      const filepath = path.join(outputDir, filename)
      
      const allHotdeals = results.flatMap(r => r.hotdeals)
      const combinedStats = this.combineStatistics(results)
      
      const exportData = {
        metadata: {
          exportDate: new Date().toISOString(),
          totalDeals: allHotdeals.length,
          source: results.map(r => r.source).join(', '),
          version: '1.0.0',
          exportedBy: 'HotdealCrawlerManager',
          crawledPages: this.options.maxPages || 2
        },
        hotdeals: allHotdeals,
        statistics: combinedStats
      }
      
      await fs.writeFile(filepath, JSON.stringify(exportData, null, 2), 'utf-8')
      exportedFiles.push(filepath)
      
      const stats = await fs.stat(filepath)
      const sizeKB = Math.round(stats.size / 1024)
      
      console.log(chalk.green(`âœ… í†µí•© ë‚´ë³´ë‚´ê¸° ì™„ë£Œ`))
      console.log(chalk.gray(`   íŒŒì¼: ${filepath}`))
      console.log(chalk.gray(`   í¬ê¸°: ${sizeKB}KB`))
      console.log(chalk.gray(`   ì´ ë”œ ìˆ˜: ${allHotdeals.length}ê°œ`))
    }
    
    return exportedFiles
  }

  async importFromJson(filepath: string): Promise<HotDeal[]> {
    console.log(chalk.cyan(`\nğŸ“¥ JSON íŒŒì¼ ê°€ì ¸ì˜¤ê¸°: ${filepath}`))
    
    try {
      const content = await fs.readFile(filepath, 'utf-8')
      const data = JSON.parse(content)
      
      if (!data.hotdeals || !Array.isArray(data.hotdeals)) {
        throw new Error('Invalid JSON format: hotdeals array not found')
      }
      
      console.log(chalk.green(`âœ… ${data.hotdeals.length}ê°œ ë”œ ê°€ì ¸ì˜¤ê¸° ì™„ë£Œ`))
      
      if (data.metadata) {
        console.log(chalk.gray(`   ì†ŒìŠ¤: ${data.metadata.source}`))
        console.log(chalk.gray(`   ë‚´ë³´ë‚¸ ë‚ ì§œ: ${data.metadata.exportDate}`))
      }
      
      return data.hotdeals
    } catch (error) {
      console.error(chalk.red('âŒ JSON ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨:'), error)
      throw error
    }
  }

  private createMetadata(result: CrawlerResult): ExportMetadata {
    return {
      exportDate: new Date().toISOString(),
      totalDeals: result.hotdeals.length,
      source: result.source,
      version: '1.0.0',
      exportedBy: 'HotdealCrawlerManager',
      crawledPages: this.options.maxPages || 2
    }
  }

  private combineStatistics(results: CrawlerResult[]): any {
    const combined = {
      totalDeals: 0,
      activeDeals: 0,
      endedDeals: 0,
      categoryCounts: {} as Record<string, number>,
      storeCounts: {} as Record<string, number>,
      sourceCounts: {} as Record<string, number>,
      freeShippingCount: 0,
      popularCount: 0,
      imagesCount: 0,
      contentCount: 0
    }
    
    for (const result of results) {
      const stats = result.statistics
      
      combined.totalDeals += stats.totalDeals
      combined.activeDeals += stats.activeDeals
      combined.endedDeals += stats.endedDeals
      combined.freeShippingCount += stats.freeShippingCount
      combined.popularCount += stats.popularCount
      combined.imagesCount += stats.imagesCount
      combined.contentCount += stats.contentCount
      
      // Merge category counts
      for (const [category, count] of Object.entries(stats.categoryCounts)) {
        combined.categoryCounts[category] = (combined.categoryCounts[category] || 0) + (count as number)
      }
      
      // Merge store counts
      for (const [store, count] of Object.entries(stats.storeCounts)) {
        combined.storeCounts[store] = (combined.storeCounts[store] || 0) + (count as number)
      }
      
      // Add source count
      combined.sourceCounts[result.source] = result.hotdeals.length
    }
    
    return combined
  }

  getSupportedSources(): string[] {
    return Array.from(this.crawlers.keys())
  }
}